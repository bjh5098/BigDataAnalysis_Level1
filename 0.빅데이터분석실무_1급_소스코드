# 정규식을 활용한 장비로그 분석하기 소스코드
setwd("c:\\temp")  
install.packages("wordcloud") 
library(wordcloud)

alert_1 <- readLines("oracle_alert_testdb.log") 

# 아래 부분이 중요합니다~!!
alert_2 <- grep("^ORA-",alert_1,value=T)
head(alert_2, 20)
 
errorcount <- table(alert_2)  
head(sort(errorcount, decreasing=T),20)

library(RColorBrewer) 
palete <- brewer.pal(9,"Set1") 
wordcloud(names(errorcount),freq=errorcount,scale=c(5,0.5),rot.per=0.5,min.freq=3,
random.order=F,random.color=T,colors=palete)


 
## ORA-12345: ~~  에서 숫자까지만 잘라내서 워드 클라우드 그리기
alert11 <- readLines("oracle_alert_testdb.log") 
alert12 <- grep("^ORA-+",alert11,value=T)  #ORA-12345 형식으로 된 줄만 걸러냄
alert13 <- substr(alert12,1,9) 
errorcount1 <- table(alert13)  
head(sort(errorcount1, decreasing=T),20)

library(RColorBrewer) 
palete <- brewer.pal(7,"Set1") 
wordcloud(names(errorcount1),freq=errorcount1,scale=c(5,0.5),rot.per=0.5,min.freq=3,
random.order=F,random.color=T,colors=palete)

#KoNLP 를 활용한 한글 텍스트 분석하기 소스코드
 setwd("c:\\temp")
#install.packages("KoNLP")
  library(KoNLP)

 txt1 <- readLines("좋아하는과일.txt")
 txt1

 txt2 <- extractNoun(txt1)
 txt2

useSejongDic( )

txt3 <- "우리는 유관순 의사와 안중근 의사가 독립투사임을 반드시 기억합시다"
extractNoun(txt3)

mergeUserDic(data.frame(c('유관순','안중근'),c('ncn')))
extractNoun(txt3)

mergeUserDic(data.frame(readLines("mergefile.txt"), "ncn"))

# 중복되는 값 제거하기
txt4 <- readLines("좋아하는과일.txt")
txt4

txt5 <- Map(extractNoun , txt4)  
txt5

txt6 <- unique(txt5)  # 중복되는 리스트를 제거합니다.
txt6

txt7 <- lapply(txt6 , unique) # 각 리스트 안에서 중복되는 단어를 제거합니다
txt7

#필요 없는 단어 제거하기 -특정 단어 지정하여 제거하기

txt7
txt8 <- gsub("최고" , "" , unlist(txt7) )
txt8

txt9 <- lapply(txt7, function(x) { gsub("최고", "", x)})
txt9

txt9 <- lapply(txt7, function(x) gsub("포도", "청포도", x) )
txt9

# 필요 없는 단어 제거하기 - 글자수로 제거하는 방법
txt9
txt10 <- lapply(txt9 ,  function(x) {
                Filter(function(y) {nchar(y) <= 6 & nchar(y) > 1 } , x ) }  )
txt10 
 

txt8
txt11 <- Filter(function(x) {nchar(x) <= 20 & nchar(x) > 1} , txt8)
txt11

# KoNLP 패키지로 한글 분석하는 종합 정리 #

#Step 1. 작업디렉토리 설정하고 필요한 패키지를 설치 및 구동합니다 
setwd("c:\\temp")  
install.packages("KoNLP") 
install.packages("wordcloud") 
install.packages("stringr")

library(KoNLP)  
library(wordcloud)
library(stringr)

useSejongDic()

#Step 2. 분석할 파일을 불러옵니다
data1 <- readLines("좋아하는과일2.txt") 
data1

#Step 3. 중복되는 리뷰를 제거해야 할 경우 아래 명령을 수행합니다. 
data1 <- unique(data1) 
data1

#Step 4. 특수 기호를 모두 제거합니다.
data1 <- str_replace_all(data1,"[^[:alpha:][:blank:]]","")
data1

#Step 5. 아래 과정이 불러온 리뷰 문장을 단어로 분리하는 과정입니다.
data2 <- extractNoun(data1)
data2

#Step 6. 한 줄 안에서 중복되는 단어를 제거해야 할 경우 아래의 명령을 수행합니다.
data2 <- lapply(data2, unique)
data2


#Step 7. 띄어 쓰기가 안되어 있는 긴 문장(단어)을 제거해야 할 경우 아래 명령을 수행합니다.
# 이 작업을 하는 명령어는 Filter( ) 함수인데 벡터로 데이터를 넣아야 합니다.
# 그래서 unlist( ) 함수로 list( ) 형태의 데이터를 벡터 형태로 변형해야 합니다.

data3 <- unlist(data2)
head(data3,5)

data4 <- Filter(function(x) {nchar(x) <= 20 & nchar(x) > 1} , data3)
data4

#Step 8. 아래 과정이 필요 없는 단어들이나 기호를 제거하는 과정입니다.
data4 <- gsub("\\.","",data4)  # 마침표 제거
data4 <- gsub("\\' ","",data4) # 홑따옴표 제거
data4 <- gsub("\\^","",data4)  # 캐럿기호 제거
data4


#Step 9. 추출된 키워드를 임시로 확인하기 위해 집계해 봅니다.
wordcount <- table(data4) 
head(sort(wordcount, decreasing=T),50)

txt <- readLines("과일gsub.txt")
cnt_txt <- length(txt)
cnt_txt

for( i in 1:cnt_txt) {
       data4 <-gsub((txt[i]),"",data4)     
     }
data4

# 위 작업을 하고 나서 삭제된 공백이나 1글자 이하의 글자를 제거하기 위해 아래 명령을 
# 수행합니다.
data4 <- Filter(function(x) {nchar(x) <= 10 & nchar(x) > 1} ,data4)
data4

#Step 10. 최종 결과를 집계하여 워드 클라우드로 시각화 합니다.
wordcount <- table(data4) 
wordcount 

head(sort(wordcount, decreasing=T),100)

palete <- brewer.pal(9,"Set3") 
wordcloud(names(wordcount),freq=wordcount,scale=c(5,1),rot.per=0.5,min.freq=1,
random.order=F,random.color=T,colors=palete)


# 예제 1. 제주 여행지 추천 키워드 분석하기 
#[ 소스 코드 ]
#Step 1. 필요한 패키지 설치 및 실행

#만약 rJava 오류가 발생할 경우
#아래 코드에서 PC 에 설치된 자바 프로그램의 경로를 수정해서 사용하세요
#Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_144')
#Sys.getenv(“JAVA_HOME”)

setwd("c:\\temp") 
install.packages("KoNLP") 
install.packages("wordcloud") 
install.packages("stringr")	# <- 패키지를 설치합니다

library(KoNLP)  
library(wordcloud)
library(stringr)  # <- 패키지를 로딩합니다

#Step 2. 사전 관련 작업
useSejongDic() 

# 위 사전에 제주도 관광지명이 정확하게 안 들어 있기 때문에 아래와 같이 수동으로 추가합니다.
mergeUserDic(data.frame(readLines("제주도여행지.txt"), "ncn"))

#Step 3. 분석할 파일을 불러와서 txt 변수에 저장합니다.
txt <- readLines("jeju.txt")  # 인코딩 오류 날 경우 encoding=”UTF-8” 사용하세요
head(txt,5)  # 반드시 잘 불러왔는지 확인 하세요

#Step 4.중복되는 행 제거하기
txt2 <- unique(txt)

#Step 5. 명사만 추출합니다
place1 <- extractNoun(txt2)
head(place1,5)

place2 <- lapply(place1, unique) # 1 줄안에서 중복되는 단어 제거하기 

#Step 6. 불용어를 찾기 위해서 현재 상태의 단어 중 많이 언급된 순서로 100개 출력확인
wordcount <- table(unlist(place2)) 
head(sort(wordcount, decreasing=T),100)

#Step 7. 불용어 제거하기
cdata <- unlist(place2) 
place3 <- str_replace_all(cdata,"[^[:alpha:][:blank:] ]","")
place3 <- gsub("랜드","", place3)
place3 <- gsub("폭포","", place3)
place3 <- gsub("주도","", place3)
place3 <- gsub("이곳","", place3)
place3 <- gsub("구석","", place3)
place3 <- gsub("생각","", place3)
place3 <- gsub("애월","애월읍", place3)
place3 <- gsub("정상","수월봉", place3)
place3 <- gsub("에코","에코랜드", place3) 
place3 <- gsub("퍼시픽","퍼시픽랜드", place3) 
place3 <- gsub(paste(c("성산","일출"),collapse='|'), "성산일출봉", place3)
place3 <- gsub(paste(c("한라","한라산"),collapse='|'), "한라산", place3)
place3 <- gsub(paste(c("산방","산방산"),collapse='|'), "산방산", place3)
place3 <- gsub(paste(c("녹차","오설록"),collapse='|'), "오설록", place3)
place3 <- gsub(paste(c("천지","지연","천지연폭포"),collapse='|'), "천지연폭포", place3)
place3 <- gsub(paste(c("주상","절리","주상절리"),collapse='|'), "주상절리", place3)
place3 <- gsub(paste(c("만장","만장굴"),collapse='|'), "만장굴", place3)

wordcount2 <- table(place3 ) 
head(sort(wordcount2, decreasing=T),100)

#불용어가 많을 경우 아래와 같이 제거합니다
txt <- readLines("제주도여행코스gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt

for( i in 1:cnt_txt) {
      place3 <-gsub((txt[i]),"",place3)     
      }

place4 <- Filter(function(x) {nchar(x) >= 2 & nchar(x) <= 10} ,place3)

wordcount3 <- table(place4) 
head(sort(wordcount3, decreasing=T),30)

# Step 8. 추출된 단어들을 워드 클라우드로 시각화 하기 
palete <- brewer.pal(7,"Set1") 
wordcloud(names(wordcount3),freq=wordcount3,scale=c(5,1),rot.per=0.25,min.freq=4,
  random.order=F,random.color=T,colors=palete)

# 참고 : Step 9. wordcloud2 ( ) 패키지로 시각화 하기
install.packages("wordcloud2")
library(wordcloud2)
wordcount4 <- head(sort(wordcount3, decreasing=T),100)
wordcloud2(wordcount4,gridSize=1,size=0.5,shape="star")

# 빅데이터 분석실무 실기시험 응시하신 분들은 아래 과정을 추가로 진행하세요.

# 아래 과정으로 생성된 본인의수험번호.csv 파일을 실기 과제물_1 로 제출해야 합니다.
# 아래의 파일들은 모두 작업 디렉토리에 저장됩니다

getwd( )  #  작업 디렉토리 확인하기
data11 <- head(sort(wordcount3, decreasing=T),40)
write.csv(data11 , '본인의수험번호.csv')

#아래 과정으로 생성된 본인의 수험번호.png 파일을 실기 과제물_2 로 제출해야 합니다. 
savePlot('본인의수험번호.png' , type='png')





